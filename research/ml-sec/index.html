<!doctype html><html lang="en" class="min-h-full antialiased"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Secure and Robust Collaborative Learning | Privacy Preserving Systems Lab</title><meta name="description" content=""><meta name="robots" content="index,follow"><link rel="preload" href="fonts/gillsans/Gill_Sans_Medium.otf"><link rel="preload" href="fonts/sourceserifpro/source-serif-pro-400-normal.woff"><link rel="preload" href="fonts/sourceserifpro/source-serif-pro-400-italic.woff"><link rel="stylesheet" href="/css/main.min.css"><script src="/js/main.min.js" defer="defer"></script><meta itemprop="name" content="Secure and Robust Collaborative Learning | Privacy Preserving Systems Lab"><meta itemprop="description" content=""><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Secure and Robust Collaborative Learning | Privacy Preserving Systems Lab"><meta name="twitter:description" content=""><meta name="twitter:site" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content="https://pps-lab.com"><meta name="og:title" content="Secure and Robust Collaborative Learning | Privacy Preserving Systems Lab"><meta name="og:description" content=""><meta name="og:image" content="https://pps-lab.com"><meta name="og:url" content="https://pps-lab.com/research/ml-sec/"><meta name="og:site_name" content="Privacy Preserving Systems Lab"><meta name="og:locale" content="en_GB"><meta name="og:type" content="website"><link rel="icon" type="image/png" href="/images/favicon/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-16x16.png"><link rel="manifest" href="/images/favicon/site.webmanifest"><link rel="canonical" href="https://pps-lab.com/research/ml-sec/"><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-4ZPP8W4RNN"></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-4ZPP8W4RNN');</script></head><body class="leading-normal bg-white dark:bg-black"><a class="sr-only" href="#main">Skip to content</a><div id="wrapper" class="wrapper pb-16 md:pb-0 flex flex-col relative min-h-screen"><div class=""><header id="header" class="inner header flex flex-col md:flex-row md:justify-start items-center p-2 md:p-4"><a class="company-logo-link block hover:text-link-hover" href="/" title="Privacy Preserving Systems Lab home"><div></div></a><nav id="nav" class="nav hidden md:flex flex-grow"><ul class="menu list-reset text-base flex flex-row justify-between flex-grow"><div class="flex flex-col md:flex-row items-center"><li class="level-1 mb-4 md:mr-8 md:mb-0"><a href="/" class="link text-black">Home</a></li><li class="level-1 mb-4 md:mr-8 md:mb-0"><a href="/people/" class="link text-black">People</a></li><li class="level-1 mb-4 md:mr-8 md:mb-0"><a href="/research/" class="link text-black">Research</a></li><li class="level-1 mb-4 md:mr-8 md:mb-0"><a href="/publications/" class="link text-black">Publications</a></li><li class="level-1 mb-4 md:mr-8 md:mb-0"><a href="/teaching/" class="link text-black">Teaching</a></li><li class="level-1 mb-4 md:mr-8 md:mb-0"><a href="/funding/" class="link text-black">Funding</a></li><li class="level-1 mb-4 md:mr-8 md:mb-0"><a href="/blog/" class="link text-black">Blog</a></li></div><li class="level-1 mb-4 md:mb-0"><a href="https://github.com/pps-lab" rel="noopener" aria-label="Github" target="_blank" class="link text-black"><svg class="fill-current" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="22" height="22" viewBox="0 0 22 22"><path d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z"></path></svg></a></li></ul></nav></header></div><main id="main" class="main inner flex flex-1 flex-col md:py-10 lg:py-10 focus:outline-none" tabindex="-1"><article id="container-centre" class="column centre flex-1"><div class="flex flex-col md:flex-row mb-8 pb-2 items-center border-b border-gray-300"><div class="w-full mx-32 text-justify"><h1 class="text-3xl page-title font-title mb-6">Secure and Robust Collaborative Learning</h1><div class="bg-white overflow-hidden float-left"><div class="md:flex flex-col"><div class="md:flex-shrink-0"><img class="h-56 object-contain w-56 m-auto" src="https://pps-lab.com//images/research/ml-sec.png" alt="'s image"></div></div></div><div class="leading-relaxed"><p>Machine learning algorithms continue to achieve remarkable success in a wide range of applications. These advancements are possible, in part, due to the availability of large domain-specific datasets, for training machine learning models. Hence, there are expanding efforts to collect more representative data to train models for new applications. This raises serious concerns regarding the privacy and security of the collected data. The privacy ramifications of massive data collection in the machine learning landscape have led both industry and academia to work on alternative privacy preserving paradigms of machine learning. Decentralized, secure machine learning have emerged as a compelling alternative for sensitive applications in the last few years. These paradigms eliminate the need to pool data centrally for ML training and thus ensure data sovereignty and alleviate the risks associated with the large-scale collection of sensitive data. Although they provide many privacy benefits, these systems make sacrifices in terms of robustness. Unfortunately, an actively malicious participant can influence the model behavior (e.g., backdoors) in these settings without being detected. As these systems are being deployed in practice for a range of sensitive applications, their robustness is growing in importance. In this project we are interested in investigating work that helps better understand and alleviate integrity issues in secure decentralized learning paradigms.</p><br><p>• <strong>RoFL</strong> (Published in IEEE S&amp;P’23) : Federated Learning (FL), though it presents many privacy benefits, amplifies ML robustness issues by exposing the learning process to an active attacker that can potentially be present throughout the entire training process and fine-tune their attacks to the current state of the model. Even though we have recently seen many attacks exposing severe vulnerabilities in FL, we still lack a holistic understanding of what enables these attacks and how they can be mitigated effectively. In our work we demystify the inner workings of existing attacks. We provide new insights on why these attacks are possible and why a definitive solution to FL robustness is challenging. We show that the inherent need for ML algorithms to memorize tail subpopulations has significant implications for ML integrity. This phenomenon has largely been studied in the context of privacy; in our analysis, we shed light on its implications for ML robustness. In addition, we show that constraints on client updates can effectively improve robustness against some severe attacks. To incorporate these constraints in secure FL protocols, we design and develop RoFL, a new secure FL system that enables constraints to be expressed and enforced on high-dimensional encrypted model updates. At its core, RoFL augments existing secure FL aggregation protocols with zero-knowledge proofs. Due to the scale of FL, realizing these checks efficiently presents a paramount challenge. We introduce several optimizations at the ML layer that allow us to reduce the number of cryptographic checks needed while preserving the effectiveness of our defenses. We build RoFL and show that it scales to model sizes as used in real FL deployments.</p><br><p>• <strong>CAMEL</strong> <em>Cryptographic Audits for Machine Learning</em>: To date, no compelling solution exists that fully addresses the robustness of secure decentralized learning paradigms. Verifiable computation and cryptographic constraints can alleviate some severe integrity issues. However, these techniques have limited effectiveness in mitigating integrity attacks that exploit memorization.This type of exploitation so far cannot be entirely prevented without impairing the ability of the model to learn fewer representative data points. As the robustness of these learning paradigms remains an open challenge, these systems need to be augmented with accountability. Towards this, we are working on cryptographic audits of decentralized secure learning systems that allow the sources of integrity issues to be traced back in a privacy-preserving way.</p><br></div></div></div><div><h2 class="text-2xl font-normal font-title">People</h2><div class="grid md:grid-cols-6 grid-cols-1 md:gap-6 gap-0"><div class="bg-white shadow-md overflow-hidden m-2 relative hover:shadow-lg transition duration-75"><div class="md:flex flex-col"><div class="md:flex-shrink-0 p-1"><div class="profile-wrapper object-center rounded-full m-auto mt-4 relative image-filter-wrap overflow-hidden"><img class="object-cover filter-grayscale" src="/images/people/hidde.jpg" alt="Hidde Lycklama"></div></div><div class="p-4 text-center"><span class="block mt-1 leading-tight font-medium text-black">Hidde Lycklama</span><p class="text-gray-700">PhD Student</p></div></div><a href="/people/hiddelycklama/" class="absolute top-0 left-0 w-full h-full overflow-hidden"></a></div><div class="bg-white shadow-md overflow-hidden m-2 relative hover:shadow-lg transition duration-75"><div class="md:flex flex-col"><div class="md:flex-shrink-0 p-1"><div class="profile-wrapper object-center rounded-full m-auto mt-4 relative image-filter-wrap overflow-hidden"><img class="object-cover filter-grayscale" src="/images/people/lukas.jpeg" alt="Lukas Burkhalter"></div></div><div class="p-4 text-center"><span class="block mt-1 leading-tight font-medium text-black">Lukas Burkhalter</span><p class="text-gray-700">PhD Student</p></div></div><a href="/people/alumni/lukasburkhalter/" class="absolute top-0 left-0 w-full h-full overflow-hidden"></a></div><div class="bg-white shadow-md overflow-hidden m-2 relative hover:shadow-lg transition duration-75"><div class="md:flex flex-col"><div class="md:flex-shrink-0 p-1"><div class="profile-wrapper object-center rounded-full m-auto mt-4 relative image-filter-wrap overflow-hidden"><img class="object-cover filter-grayscale" src="/images/people/nicolas.jpeg" alt="Nicolas Küchler"></div></div><div class="p-4 text-center"><span class="block mt-1 leading-tight font-medium text-black">Nicolas Küchler</span><p class="text-gray-700">PhD Student</p></div></div><a href="/people/nicolaskuechler/" class="absolute top-0 left-0 w-full h-full overflow-hidden"></a></div><div class="bg-white shadow-md overflow-hidden m-2 relative hover:shadow-lg transition duration-75"><div class="md:flex flex-col"><div class="md:flex-shrink-0 p-1"><div class="profile-wrapper object-center rounded-full m-auto mt-4 relative image-filter-wrap overflow-hidden"><img class="object-cover filter-grayscale" src="/images/people/alex.png" alt="Alexander Viand"></div></div><div class="p-4 text-center"><span class="block mt-1 leading-tight font-medium text-black">Alexander Viand</span><p class="text-gray-700">Making FHE accessible @ Intel Labs</p></div></div><a href="/people/alexanderviand/" class="absolute top-0 left-0 w-full h-full overflow-hidden"></a></div><div class="bg-white shadow-md overflow-hidden m-2 relative hover:shadow-lg transition duration-75"><div class="md:flex flex-col"><div class="md:flex-shrink-0 p-1"><div class="profile-wrapper object-center rounded-full m-auto mt-4 relative image-filter-wrap overflow-hidden"><img class="object-cover filter-grayscale" src="/images/people/anwar.jpg" alt="Anwar Hithnawi"></div></div><div class="p-4 text-center"><span class="block mt-1 leading-tight font-medium text-black">Anwar Hithnawi</span><p class="text-gray-700">Group Leader</p></div></div><a href="/people/anwarhithnawi/" class="absolute top-0 left-0 w-full h-full overflow-hidden"></a></div></div></div><div class="mt-6"><h2 class="text-2xl font-normal font-title">Publications</h2><div class="mb-8 text-lg publication"><div class="flex flex-col md:flex-row items-center"><div class="w-0 md:w-20 flex-shrink-0 border-2 max-h-24"><a href="https://arxiv.org/abs/2402.15780" target="_blank" rel="noopener"><img alt="Thumbnail of Holding Secrets Accountable: Auditing Privacy-Preserving Machine Learning" src="/images/papers/paper_arc_thumbnail.jpg"></a></div><div class="md:ml-6 leading-snug"><h4 class="mb-0 mt-0"><span class="mr-4 font-medium">Holding Secrets Accountable: Auditing Privacy-Preserving Machine Learning</span><a href="https://arxiv.org/abs/2402.15780" target="_blank" class="publication-icon" rel="noopener"> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path></svg> <span>Paper</span></a></h4><p><a href="/people/hiddelycklama"><span>Hidde Lycklama</span></a>, <a href="/people/alexanderviand"><span>Alexander Viand</span></a>, <a href="/people/nicolaskuechler"><span>Nicolas Küchler</span></a>, <a href="https://cknabs.github.io/" target="_blank" rel="noopener"><span>Christian Knabenhans</span></a>, <a href="/people/anwarhithnawi"><span>Anwar Hithnawi</span></a></p><p class="text-red-600 font-bold"></p><p>USENIX Security 2024.</p></div></div></div><div class="mb-8 text-lg publication"><div class="flex flex-col md:flex-row items-center"><div class="w-0 md:w-20 flex-shrink-0 border-2 max-h-24"><a href="https://arxiv.org/abs/2107.03311" target="_blank" rel="noopener"><img alt="Thumbnail of RoFL: Robustness of Secure Federated Learning" src="/images/papers/paper_fl_project_thumbnail.jpg"></a></div><div class="md:ml-6 leading-snug"><h4 class="mb-0 mt-0"><span class="mr-4 font-medium">RoFL: Robustness of Secure Federated Learning</span><a href="https://arxiv.org/abs/2107.03311" target="_blank" class="publication-icon" rel="noopener"> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path></svg> <span>Paper</span> </a><a href="https://github.com/pps-lab/rofl-project-code" target="_blank" class="publication-icon" rel="noopener"><svg class="fill-current" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="24" height="24" viewBox="0 0 24 24"><path d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z"></path></svg> <span>Github</span></a></h4><p><a href="/people/hiddelycklama"><span>Hidde Lycklama</span>*</a>, <a href="/people/alumni/lukasburkhalter"><span>Lukas Burkhalter</span>*</a>, <a href="/people/alexanderviand"><span>Alexander Viand</span></a>, <a href="/people/nicolaskuechler"><span>Nicolas Küchler</span></a>, <a href="/people/anwarhithnawi"><span>Anwar Hithnawi</span></a></p><p class="text-red-600 font-bold"></p><p>IEEE Security and Privacy (Oakland) 2023.</p></div></div></div><div class="mb-8 text-lg publication"><div class="flex flex-col md:flex-row items-center"><div class="w-0 md:w-20 flex-shrink-0 border-2 max-h-24"><a href="https://openreview.net/forum?id=vNrSXIFJ9wz" target="_blank" rel="noopener"><img alt="Thumbnail of VF-PS: How to Select Important Participants in Vertical Federated Learning, Efficiently and Securely?." src="/images/papers/paper_timecrypt_thumbnail.jpg"></a></div><div class="md:ml-6 leading-snug"><h4 class="mb-0 mt-0"><span class="mr-4 font-medium">VF-PS: How to Select Important Participants in Vertical Federated Learning, Efficiently and Securely?.</span><a href="https://openreview.net/forum?id=vNrSXIFJ9wz" target="_blank" class="publication-icon" rel="noopener"> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path></svg> <span>Paper</span></a></h4><p>Jiawei Jiang, <a href="/people/alumni/lukasburkhalter"><span>Lukas Burkhalter</span></a>, Fangcheng Fu, Bolin Ding, Bo Du, <a href="/people/anwarhithnawi"><span>Anwar Hithnawi</span></a>, Bo Li, Ce Zhang</p><p class="text-red-600 font-bold"></p><p>NeurIPS (Spotlight) 2022.</p></div></div></div><div class="mb-8 text-lg publication"><div class="flex flex-col md:flex-row items-center"><div class="w-0 md:w-20 flex-shrink-0 border-2 max-h-24"><a href="https://pps-lab.com/papers/camel_mlsafety.pdf" target="_blank" rel="noopener"><img alt="Thumbnail of Cryptographic Auditing for Collaborative Learning" src="/images/papers/paper_camel_mlsafety_thumbnail.jpg"></a></div><div class="md:ml-6 leading-snug"><h4 class="mb-0 mt-0"><span class="mr-4 font-medium">Cryptographic Auditing for Collaborative Learning</span><a href="https://pps-lab.com/papers/camel_mlsafety.pdf" target="_blank" class="publication-icon" rel="noopener"> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path></svg> <span>Paper</span></a></h4><p><a href="/people/hiddelycklama"><span>Hidde Lycklama</span></a>, <a href="/people/nicolaskuechler"><span>Nicolas Küchler</span></a>, <a href="/people/alexanderviand"><span>Alexander Viand</span></a>, <a href="https://emanuelopel.ch/" target="_blank" rel="noopener"><span>Emanuel Opel</span></a>, <a href="/people/alumni/lukasburkhalter"><span>Lukas Burkhalter</span></a>, <a href="/people/anwarhithnawi"><span>Anwar Hithnawi</span></a></p><p class="text-red-600 font-bold"></p><p>ML Safety Workshop at NeurIPS 2022</p></div></div></div></div></article></main><div class="footer-wrapper"><footer id="footer" class="footer"><div class="inner m-auto md:flex justify-between items-center py-10"><a href="https://ethz.ch" class="block text-gray-500 hover:text-pink-500" target="_blank" rel="noopener noreferrer"><span class="sr-only">ETH</span> <img src="https://pps-lab.com/images/logo/eth_logo_cropped.png" alt="ETH logo" class="h-10"></a></div></footer></div></div><nav id="mobile-nav" class="mobile-nav fixed left-0 top-0 h-screen w-full overflow-y-auto pt-12 bg-gray-200 z-50 md:hidden"><ul><li class="item relative opacity-0"><a href="/" class="link block py-2 px-5 text-dark text-3xl hover:text-link-hover">Home</a></li><li class="item relative opacity-0"><a href="/people/" class="link block py-2 px-5 text-dark text-3xl hover:text-link-hover">People</a></li><li class="item relative opacity-0"><a href="/research/" class="link block py-2 px-5 text-dark text-3xl hover:text-link-hover">Research</a></li><li class="item relative opacity-0"><a href="/publications/" class="link block py-2 px-5 text-dark text-3xl hover:text-link-hover">Publications</a></li><li class="item relative opacity-0"><a href="/teaching/" class="link block py-2 px-5 text-dark text-3xl hover:text-link-hover">Teaching</a></li><li class="item relative opacity-0"><a href="/funding/" class="link block py-2 px-5 text-dark text-3xl hover:text-link-hover">Funding</a></li><li class="item relative opacity-0"><a href="/blog/" class="link block py-2 px-5 text-dark text-3xl hover:text-link-hover">Blog</a></li></ul></nav><button id="mobile-nav-toggle" class="mobile-nav-toggle block fixed h-16 w-full bottom-0 flex items-center justify-center font-bold border-none bg-gray-200 text-dark z-50 focus:outline-none md:hidden" aria-label="Mobile menu toggle" aria-expanded="false" aria-controls="mobile-nav"><span class="mobile-nav-label mr-2 font-medium">Menu</span> <span class="mobile-nav-icon" aria-hidden="true"><span class="mobile-nav-icon-line bg-black w-6 block transition-all duration-200 ease-out mb-1"></span> <span class="mobile-nav-icon-line bg-black w-6 block transition-all duration-200 ease-out mb-1"></span> <span class="mobile-nav-icon-line bg-black w-6 block transition-all duration-200 ease-out"></span></span></button></body></html>